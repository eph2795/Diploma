\documentclass[14pt, a4paper, oneside, bold]{extarticle}

\usepackage[english, russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage[14pt]{extsizes}
\usepackage{listingsutf8}
\usepackage{color}
\usepackage{wrapfig}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage{indentfirst}

\usepackage{setspace}
\singlespacing
\onehalfspacing
\doublespacing

\allowdisplaybreaks
\binoppenalty = 10000
\relpenalty = 10000
\textheight = 23cm
\textwidth = 20cm
\oddsidemargin = 0pt
%\topmargin = -1.5cm
\parskip = 0pt
\tolerance = 2000
\flushbottom

\usepackage[left=30mm, right=20mm, top=20mm, bottom=30mm, nohead, footskip=10mm]{geometry}
% bottom=2cm, bindingoffset=0cm]

\pagestyle{plain}

\renewcommand{\thesection}{\arabic{section}}

\begin{document}

\allowdisplaybreaks[1]

\begin{titlepage}
\setstretch{1}
\begin{center}
\ \vspace{-1.5cm}

\includegraphics[width=0.5\textwidth]{msu_logo.eps}\\
{\bfseries Московский государственный университет имени М.В. Ломоносова \\
Факультет вычислительной математики и кибернетики\\
Кафедра исследования операций}

\vspace{3cm}

{\Large Лаврухин Ефим Валерьевич}

\vspace{1cm}

{\Large\bfseries
Применение нейронных сетей для сегментации томографических изображений геологических пород\\}

\vspace{1cm}

{\textbf \large МАГИСТЕРСКАЯ ДИССЕРТАЦИЯ}
\end{center}

\vfill

\begin{flushright}
  \textbf{Научный руководитель:}\\
  к.ф.-м.н., доцент\\
  Д.В.~Денисов
\end{flushright}

\vfill

\begin{center}
Москва, 2018
\end{center}

\vspace{1cm}

\enlargethispage{2\baselineskip}

\end{titlepage}


\setstretch{1.5}

\newpage

\section*{Введение}

\textbf{Актуальность темы исследования.} В настоящее время добыча полезных ископаемых требует большое количество 
данных о разрабатываемых породах-коллекторах. Эти данные получают в том числе с помощью методов цифровой петрофизики, которые работают с  
2-D или 3-D изображениями, сделанными с помощью рентгеновской томографии \cite{1}. Большинство изображений строения пород представлены в градациях серого, которые указывают на интенсивность поглощения рентгеновских лучей. На практике любой метод численного расчета характеристик исходных пород состоит из нескольких отдельных этапов. 
И первый этап -- это сегментация входного изображения, разделение его
на несколько различных фаз по плотности вещества. В простейшем случае выполняется бинаризация -- разделение на твёрдую породу и поры \cite{2}. 

\textbf{Цель} данной работы -- применить методы глубинного обучения, а именно полносвёрточные нейронные сети, для задачи сегментации изображений геологических пород.

\textbf{Научная новизна.} В настоящее время существует большое количество методов сегментации. Они существенно отличаются в используемых предположениях о входных изображениях и математическом аппарате. Вот некоторые из них: градиентные \cite{16}, морфологические, случайные поля \cite{14}, \cite{15}, методы Монте-Карло \cite{13}. У этих методов есть ряд достоинств: присутствует математическая формализация, относительная простота постановки задачи, интерпретируемость результатов. Но в то же время все они обладают серьёзным недостатком -- в них присутствуют гиперпараметры, которые сильно влияют на качество результата. Это делает затруднительным их применение без оператора, который контролирует процесс и подбирает нужные значения параметров для  конкретных входных данных. 

Относительно недавно появились методы сегментации с использованием машинного обучения. Они так же применяются и в сегментации изображений геологических пород \cite{3}, \cite{4}, \cite{5}. Глубинное обучение - это подкласс моделей машинного обучения, в которых используется сложная многоуровневая композиция слоёв для извлечения нелинейных признаков исходного объекта. Главные преимущества этих моделей - это высокое качество(выше, чем у других методов, решающих аналогичные задачи) и полная автономность обучения(для того, чтобы построить качественную модель на имеющихся данных не требуется участие человека).

Сейчас свёрточные нейронные сети являются, фактически, state-of-the-art в задачах обрабоки изображений \cite{6} и используются во многих прикладных областях: биологии, медицине, распознавании образов \cite{11}, \cite{12}. На текущий момент выпущено достаточно много работ, в которых исследуются методы глубинного обучения для решения связанных с сегментацией пород задач. В частности для  построения стохастической реконструции пород с последующим моделированием физических свойств \cite{7}, \cite{8}, \cite{9}, \cite{10}. Но статей, в которых глубинное обучение применяется для сегментации геологических пород, крайне мало. 

В ходе работы для достижения поставленной цели решались следующие 
\textbf{задачи}:
\begin{enumerate}
	\item Выбор полносвёртночной архитектуры нейронной сети, 
	которая выполняет сегментацию изображений томограмм.
	\item Решение проблемы отсутствия размеченных обучающих данных.
	\item Построение стабильного алгоритма обучения сети.
	\item Учёт внутри модели 3-D структуры входных данных.
	\item Сравнение полученных результатов с результатами других 			моделей для сегментации геологических изображений.
	\item Сравнение физических характеристик, полученных с помощью 
	отсегментированных моделью изображений, с характеристиками 				исходного образца, вычисленными с помощью физических симуляций.
\end{enumerate}
  
\newpage


\section{Постановка задачи}

Дано исходное изображение 
	$S = (s_{ij})_{i=1, j=1}^{H, W},\ s_{ij} \in [0, 1]$, где 
$H$ - высота изображения, $W$ - ширина изображения. 
Требуется для каждого пикселя найти соответствующий ему сегмент изображения, 
т.е. найти соответствие $s_{ij} \rightarrow m_{ij},\ m_{ij} \in C = \{ 0, ... , N_c-1 \} $,
где $N_c$ - число сегментов.

Подразумевается, что для каждого изображения $S$ существует 
истинное(возможно, не одно) сегментированное изображение $M$.
Поэтому можно перейти к следующей постановке задачи: найти алгоритм сегментации $\mathcal{A}$, такой, что он преобразует любое 
изображение $S$ в  маску $\hat{M}$:
\begin{equation}
	\mathcal{A}(S | \theta) = \hat{M}
\end{equation} 
, где $\theta$ - настраеваемые параметры нашего алгоритма.

Возникают следующие вопросы:
\begin{enumerate}
	\item Как выбрать алгоритм $\mathcal{A}$?
	\item Как настроить параметры $\theta$?
	\item Как оценить ошибку алгоритма?
\end{enumerate}

В данном случае наши параметры $\theta$ подбираеются с помощью 
обучения без учителя(unsupervised learning), либо непосредственно 
экспериментатором.

\newpage


\section{Особенности задачи сегментации геологических пород}

Дан исходный стек изображений 
	$S = (s_{ijk})_{i=1, j=1, k=1}^{H, W, D},\ s_{ijk} \in [0, 1]$, где 
$H$ - высота изображения, $W$ - ширина изображения, $D$ - количество изображений. 
Требуется для каждого пикселя найти соответствующую ему метку класса, 
т.е. найти соответствие $s_{ijk} \rightarrow m_{ijk},\ m_{ijk} \in C = \{ 0, 1 \} $, где класс $0$ соответствует порам, класс $1$ - твердому веществу.

Особенности данной задачи:
\begin{enumerate}
	\item Работа с 3-D изображениями.
	\item Двухклассовая сегментация.
	\item Наличие сегментированных стеков, для которых изместно 
	$\hat{M}$ некоторого ''качественного'' алгоритма.	
\end{enumerate}

Как правило, данная задача на практике решалась методами MRF, snakes и некоторыми другими. Собрана некоторая база изображений 
$S, \hat{M}$, качество сегментации которых оценивалось оператором.

В таком случае естественно перейти к задаче обучения с учителем(supervised learning), чтобы использовать накопившуюся библиотеку сегментированных изображений. 

\newpage


\section{Постановка задачи supervised сегментации}


Дано пространство объектов $X$ и пространтсво ответов $Y$. Между ними существует соответствие(функция) $f: X \rightarrow Y$. 

Требуется наилучшим образом приблизить соответствие $f$ при помощи 
параметрического семества функций $f_{\theta}$ и множеством примеров отображения $f: \{(X_1,\ Y_1),\ ... ,\ (X_N,\ Y_N) \}
	,\ f(X_i) = Y_i ,\ i = \overline{1, N}$.

В конкретном случае множество примеров из пространства $X$ задаётся в виде:
\begin{equation}
	\hat{X} = \{ X_1, ..., X_N \},\ X_k = (x_{ij})_{i=1, j=1}^{H, W}
                       	   ,\ x_{ij} \in [0, 1]
\end{equation}
и множество ответов из $Y$ задаётся в виде:
\begin{equation}
	\hat{Y} = \{ Y_1, ... , Y_N \},\ Y_k = (y_{ij})_{i=1, j=1}^{H, W}
							,\ y_{ij} \in \{ 0, 1 \}.
\end{equation} 

Конкретное значение параметра $\theta$ параметрического семейства $f_{\theta}$ выбирается изходя из функционала качества модели(функции эмпирического риска):
\begin{equation}\label{opt}
	Q \bigl( \theta, (\hat{X}, \hat{Y}) \bigr) = 
		\sum \limits_{i=1}^{N} \mathcal{L} 
		\bigl( f_{\theta}(X_i), Y_i \bigr)
		\longrightarrow \min \limits_{\theta}.
\end{equation}

Задача состоит в выборе параметрического семейства $f_\theta$, 
функции потерь $\mathcal{L}(\overline{Y}, Y)$ и решении 
оптимизационной задачи (\ref{opt}).

\newpage


\section{Архитектура нейронной сети}

В качестве семейства функций $f_{\theta}$ в работе использовалась 
полносвёрточная нейронная сеть (fully-connected convolutional neutral network). В качестве основы была выбрана архитектура U-net, которая зарекомендовала себя в решении задач биологии(выделение границ клеток). 

В архитектуру были внесены незначительные изменения: уменьшено количество свёрточных фильтров, добавлен padding, изменена функция активации на ELU.

Сеть представляет из себя композицию линейных
(свертки (\ref{conv}), конкатенация (\ref{concat}), и transposed convolutions, действие которых эквивалентно обратному действию свёрточных слоев, т.е по выходу свёрточного слоя $y$ и фильтрам $w$ получается вход свёрточного слоя $x$) 
и нелинейных преобразований
(активации (\ref{ELU}), (\ref{softmax}), pooling (\ref{pooling})) которые применяются последовательно. 
Вероятности на выходе обеспечиваются с помощью 
softmax-преобразования выхода сети: 

\begin{equation} \label{ELU}
	y = \begin{cases} 
		& x ,\ \text{при $x \geq 0$}, \\	
		& e^x - 1 ,\ \text{при $x < 0$}.
	\end{cases}
\end{equation}

\begin{equation} \label{softmax}
\begin{aligned}
	& y_{c, i, j} = \frac{ e^{x_{c, i, j}} }
		{ \sum \limits_{l=1}^{C} e^{x_{l, i, j}} } \\
	& x \in \mathbb{R}^{C \times W \times H}
		,\ y \in \mathbb{R}^{C \times W \times H}
\end{aligned}
\end{equation}

\begin{equation} \label{conv}
\begin{aligned}
	y_{c, i, j} & = \sum \limits_{l=1}^{C} 
		\sum \limits_{m=1}^{min(W - i, K)}
		\sum \limits_{n=1}^{min(H - j, K)} 
		x_{l, i + m, j + n} \ w^c_{l, m, n} \\
	& ,\ c = \overline{1, T} 
		,\ i = \overline{1, W}
		,\ j = \overline{1, H} \\
	& \text{, где $x \in \mathbb{R}^{C \times W \times H}$
		- вход свёртки} \\
	& \text{, $y \in \mathbb{R}^{T \times W \times H}$ 
		- выход свёртки} \\
	& \text{, $w^c \in \mathbb{R}^{C \times K \times K}$
		- фильтры свёрток} \\
	& \text{, $c=\overline{1, C}$ - количество свёрток}.
\end{aligned}
\end{equation}

\begin{equation} \label{concat}
\begin{aligned}
	& z_{c, i, j} = \begin{cases}
		& x_{c, i, j} \text{, если $c \leq C$,} \\
		& y_{c - C, i, j} \text{, если $c > C$}
	\end{cases} \\
	& \text{, где $x \in \mathbb{R}^{C \times W \times H}$
	,\ $y \in \mathbb{R}^{T \times W \times H}$ 
	,\ $z \in \mathbb{R}^{(T + C) \times W \times H}$}
\end{aligned}
\end{equation}

\begin{equation} \label{pooling}
\begin{aligned}
	y_{c, i, j} & = \max \limits_{
		\substack{
			m=\overline{1, min(W - i, K)}
			\\ n=\overline{1, min(H - j, K)}}}
		x_{c, i K + m, j K + n} \\
	& \text{, где $x \in \mathbb{R}^{C \times W \times H}$ 
		- вход pooling'а} \\
	& \text{, $y \in \mathbb{R}
		^{C 
		  \times \bigl[ \frac{W + K - 1}{K} \bigr] 
		  \times \bigl[ \frac{H + K - 1}{K} \bigr] }$ 
		- выход pooling'а} \\
	& \text{, $K$ - размер ядра pooling'а}.
\end{aligned}
\end{equation}

Модель реализует следующее отображение: 
\begin{equation} \label{model}
\begin{aligned}
	& f_{\theta}(x) = y, \\
	& x \in \mathbb{R}^{1 \times H \times W}
		,\ y \in 
		\mathbb{R}^{2 \times H \times W}
\end{aligned}
\end{equation}
, где первая первая размерность - число каналов изображения(на входе один, потому что изображение grayscale; на выходе два: в первом канале вероятность того, что текущий пиксель - это пора, во втором - что это твёрдое вещество), последние две размерности - это размер изображений. В качестве оптимизируемых параметров модели $\theta$ выступает совокупность весов convolutional и transposed convolutional слоёв.

\newpage


\section{Оптимизация модели}

Для финальной постановки задачи осталось выбрать функцию потерь 
$\mathcal{L}(\overline{Y}, Y)$. Поскольку выход модели (\ref{model})
является бинарными вероятностями, подходящей функцией является кросс-энтропия:
\begin{equation} \label{cross-entropy}
	CE(\overline{Y}, Y) = \sum \limits_{i=1, j=1}^{W, H} \Bigl(
		Y_{ij} \log \overline{Y}_{0ij} 
		+ (1 - Y_{ij}) \log (1 - \overline{Y}_{1ij}) \Bigr).
\end{equation}

Выбор обуславливается тем, что функция гладкая, выпуклая, минимум достигается 
при выборе с помощью прогноза $\overline{Y}$ верного класса и функция 
имеет адекватную вероятностную интерпретацию.

Так же в качестве метрики качества в задачах сегментации используют коэффициент Жаккара: 
\begin{equation} \label{jaccard}
	J(A, B) = \frac{|A \cap B|}{|A \cup B|} 
\end{equation}
, где в качестве множеств $A$ и $B$ выступают множество пикселей, отнесённые к 
первому(второму) классу моделью, и множество пикселей, образующих правильный ответ для первого(второго) класса.

Коэффициент Жакарра (\ref{jaccard}) нельзя использовать для обучения модели напрямую, потому что для его вычисления требуется определённый(метка конкретного класса) прогноз модели, а модель 
(\ref{model}) возвращает вероятности меток. При переходе от вероятности к меткам по порогу(например, при $p<0.5$ предсказывается первый класс, иначе - второй) функция перестаёт быть гладкой. Поэтому в качестве функции потерь можно использовать гладкую аппроксимацию (\ref{jaccard}). Можно придумать различные виды аппроксимации, в данной работе применялась следующая:
\begin{equation} \label{smoothed_iou}
	sIOU(\overline{Y}, Y) = \sum \limits_{i=1, j=1}^{W, H}
		\frac{Y_{ij} \overline{Y}_{ij} + \varepsilon}
		{Y_{ij} + \overline{Y}_{ij} 
			- Y_{ij} \overline{Y}_{ij} + \varepsilon}.
\end{equation} 
 
Можно убедиться, что при ''стремлении'' 
$\overline{Y}$ к $Y$ значение $sIOU(\overline{Y}, Y)$ стремится к $J(A(\overline{Y}, \tau), Y)$ при любом выборе порога $\tau$. В выражении (\ref{smoothed_iou}) фигурирует 
константа $\varepsilon$(на практике, например, $\varepsilon=10^{-5}$), которая используется для вычислительной стабильности.
 
Итоговая фунция потерь, которая использовалась для оптимизации модели, имеет вид:
\begin{equation} \label{final_loss}
	\mathcal{L}(\overline{Y}, Y) = CE(\overline{Y}, Y)
		+ \alpha \log \bigl( sIOU(\overline{Y}, Y) \bigr)
\end{equation}
, где $\alpha$ - коэффициент соотношения функций потерь. Величина (\ref{smoothed_iou}) находится под логарифмом для коррекции соотношения к величине (\ref{cross-entropy}).

\newpage


\section{Заключение}


\addcontentsline{toc}{chapter}{Литература}
\begin{thebibliography}{99}

\bibitem{1} S. Karimpoulia
	, P. Tahmasebib,
	, H. L. Ramandic
	, P. Mostaghimid
	, M. Saadatfar, 
	``Stochastic modeling of coal fracture network by direct use of microcomputed
tomography images'', International Journal of Coal Geology 179, 153-163, 2017.

\bibitem{2} Jeff T. Gostick, 
	``Versatile and efficient pore network extraction method using marker-based watershed segmentation'', Physical Review E 96, 2017.

\bibitem{3} S. Chauhan
	, W. Rühaak,
	, H. Anbergen
	, A. Kabdenov
	, M. Freise
	, T. Wille
	, I. Sass,
	``Phase segmentation of X-ray computer tomography rock images
using machine learning techniques: an accuracy
and performance study'', Solid Earth, 7, 1125–1139, 2016.
	
\bibitem{4} F. Khan
	, F. Enzmann,
	, M. Kersten, 
	``Multi-phase classification by a least-squares support vector machine
approach in tomography images of geological samples'', Solid Earth, 7, 481–492, 2016.

\bibitem{5} S. Chauhan
	, W. Rühaak,
	, F. Khan
	, F. Enzmann
	, P. Mielke
	, I. Sass,
	``Processing of rock core microtomography images: Using seven
different machine learning algorithms'', Computers \& Geosciences, 86, 120-128, 2016.

\bibitem{6} O. Ronneberger
	, P. Fischer
	, T. Brox,
	``U-Net: Convolutional Networks for Biomedical
Image Segmentation'', arXiv:1505.04597v1, 2015.

\bibitem{7} L. Mosser
	, O. Dubrule,
	, Martin J. Blunt, 
	``Reconstruction of three-dimensional porous media
using generative adversarial neural networks'', 
arXiv:1704.03225v1, 2017

\bibitem{8} Brian L. DeCost
	, T. Francis
	, Elizabeth A. Holm, 
	``Exploring the microstructure manifold: image
texture representations applied to ultrahigh 
carbon steel microstructures'', 
arXiv:1702.01117v2, 2017

\bibitem{9} Ruijin Cang
	, Yaopengxiao Xu
	, Shaohua Chen
	, Yongming Liu
	, Yang Jiao
	,M. Yi Ren,
	`Microstructure Representation and
Reconstruction of Heterogeneous Materials via
Deep Belief Network for Computational Material 
Design'', 
arXiv:1612.07401v3, 2017

\bibitem{10} N. Lubbers
	, T. Lookman
	, K. Barros,
	``Inferring low-dimensional microstructure representations using
convolutional neural networks'', 
arXiv:1611.02764v1, 2016

\bibitem{11} A. S. Razavian
	, H. A. Josephine
	, S. S. Carlsson,
	``CNN Features off-the-shelf: an Astounding Baseline for Recognition'', 
arXiv:1403.6382v3, 2014.

\bibitem{12} Kwang Moo Yi
	, Eduard Trulls
	, Vincent Lepetit
	, Pascal Fua,
	``LIFT: Learned Invariant Feature Transform'', 
arXiv:1603.09114v2, 2016.

\bibitem{13} С.А. Эль-Хатиб,
	``Сегментация изображений с мопощью смешаного и экспоненциального алгоритмов роя частиц'', 
Информатика и кибернетика, 1, 2015.

\bibitem{14} H. Deng
	, D.A. Clausi, 
	``Unsupervised image segmentation using a simple MRF model with a new implementation scheme'', 
Pattern Recognition, 37, 2323-2335, 2004.

\bibitem{15} H. Deng
	, D.A. Clausi, 
	``Image segmentation using Markov Random Field Model in Fully Parallel Cellular Nerwork Architecture'', 
Real-time Imaging, 6, 195-211, 2000.

\bibitem{16} H. Deng
	, D.A. Clausi, 
	``Improved Workflow for Unsupervised Multiphase Image Segmentation'', 
arXiv:1710.0967, 2017.

\end{thebibliography}

\end{document}
